# Encoder Architecture

## BERT

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [BERT 101](https://huggingface.co/blog/bert-101)
- [How to Code BERT Using PyTorch â€“ Tutorial With Examples](https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial)
- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

## RoBERTa

- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [Training RoBERTa from scratch - the missing guide](https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/)
