# Model Compression

## Pruning

- [Pruning Tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)

## Distillation

- [Knowledge Distillation Tutorial](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)
- [Distilling Llama3.1 8B into Llama3.2 1B using Knowledge Distillation](https://pytorch.org/torchtune/0.3/tutorials/llama_kd_tutorial.html)
- [Distilling Llama3.1 8B into 1B in torchtune](https://pytorch.org/blog/llama-into-torchtune/)

## Quantization

- [Quantization](https://pytorch.org/docs/stable/quantization.html)
- [Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)
- [Quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)
- [Quantization](https://huggingface.co/docs/transformers/en/quantization/overview)
- [Quantize ü§ó Transformers models](https://huggingface.co/docs/transformers/v4.27.0/en/main_classes/quantization)
- [Quantization-Aware Training for Large Language Models with PyTorch](https://pytorch.org/blog/quantization-aware-training/)
- [Introduction to Quantization cooked in ü§ó with üíóüßë‚Äçüç≥](https://huggingface.co/blog/merve/quantization)