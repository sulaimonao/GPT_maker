# Inference Cost and Speed Optimization

## Distillation

- [LLM distillation demystified: a complete guide](https://snorkel.ai/blog/llm-distillation-demystified-a-complete-guide/)
- [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/pdf/2305.02301)
- [MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/pdf/2306.08543)
- [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/pdf/2402.13116)
- [Knowledge distillation: Teaching LLM's with synthetic data](https://wandb.ai/byyoung3/ML_NEWS3/reports/Knowledge-distillation-Teaching-LLM-s-with-synthetic-data--Vmlldzo5MTMyMzA2)

## Pruning

- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627)
- [A Simple and Effective Pruning Approach for Large Language Models](https://arxiv.org/abs/2306.11695)
- [The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction](https://arxiv.org/pdf/2312.13558)
- [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](https://arxiv.org/abs/2310.06694)

## Speculative Decoding

- [A Hitchhiker's Guide to Speculative Decoding](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318)
- [Inference with Reference: Lossless Acceleration of Large Language Models](https://arxiv.org/pdf/2304.04487)
- [Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding](https://arxiv.org/pdf/2307.05908)
- [Accelerating LLM Inference with Staged Speculative Decoding](https://arxiv.org/pdf/2308.04623)
- [Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/)
- [SpecTr: Fast Speculative Decoding via Optimal Transport](https://openreview.net/pdf?id=SdYHLTCC5J)

## Post-Training Quantization

- [Quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)
- [Top LLM quantization methods and their impact on model quality](https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/)
- [Doing more with less: LLM quantization](https://www.redhat.com/en/blog/doing-more-less-llm-quantization-part-2)
- [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf)
- [A Comprehensive Study on Post-Training Quantization for Large Language Models](https://cli99.com/pdf/ds-w4a16-23.pdf)
- [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
- [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717)
- [RPTQ: Reorder-based Post-training Quantization for Large Language Models](https://arxiv.org/pdf/2304.01089)