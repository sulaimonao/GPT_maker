# The Hundred-Page Language Models Book's Wiki

[gimmick:theme](spacelab)

## Getting Started

- [PyTorch tutorials](PyTorch.md)
- [Math fundamentals](math.md)
- [GPU-enabled notebook services](notebook-services.md)
- [GPU rental services](GPU-rental.md)

## Extended Chapters

* [Derivation for 1.7 Gradient Descent](https://www.dropbox.com/scl/fi/c7mis58frqk5n88dhxjmb/chapter_1_extra_1.pdf?rlkey=rbhrgjl0lmz31btl9n1ahc56w&dl=0)

## Code

* [Python scripts](scripts.md)
* [Colab notebooks](colabs.md)

## Engineering

* [Online finetuning services](online-finetuning.md)
* [Deployment](deployment.md)
* [Inference cost and speed optimization](inference.md)
* [Distributed training](distributed.md)
* [Preventing overfitting](overfitting.md)

## Language Model

* [Evaluation methods](evaluation.md)
* [Prompt engineering](prompting.md)
* [Function calling](function-calling.md)

## Advanced Topics

* [Scaling laws](scaling.md)
* [Mixture of experts](MoE.md)
* [Model merging](merging.md)
* [Model compression](compression.md)
* [Preference-based alignment](alignment.md)
* [Security](security.md)
* [Vision language models](VLM.md)

## Additional Reading

* [Embeddings](embeddings.md)
* [Tokenization methods](tokenization.md)
* [Encoder architecture](encoder.md)
* [Encoder-decoder architecture](encoder-decoder.md)
* [Non-Transformer architectures](non-transformer.md)
