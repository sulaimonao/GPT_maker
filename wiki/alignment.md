# Alignment

## RLHF

- [Reinforcement Learning from Human Feedback](https://icml.cc/media/icml-2023/Slides/21554.pdf)
- [A curated list of reinforcement learning with human feedback resources](https://github.com/opendilab/awesome-RLHF)

## DPO

- [Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)
- [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl)
- [RLHF in 2024 with DPO & Hugging Face](https://www.philschmid.de/dpo-align-llms-in-2024-with-trl)
- [Unveiling the Hidden Reward System in Language Models: A Dive into DPO](https://allam.vercel.app/post/dpo/)